{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP1iKQVgaOCWTl+Xr/2RGCH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sDEm3v-Lc7zY"},"outputs":[],"source":["# Evaluation"]},{"cell_type":"code","source":["### Technically, there are very few steps to run it on GPUs, elsewhere (ie. on Lamini).\n","```\n","finetuned_model = BasicModelRunner(\n","    \"lamini/lamini_docs_finetuned\"\n",")\n","finetuned_output = finetuned_model(\n","    test_dataset_list # batched!\n",")\n","```\n","\n","### Let's look again under the hood! This is the open core code of Lamini's `llama` library :)"],"metadata":{"id":"PxZlaB0uhUbV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datasets\n","import tempfile\n","import logging\n","import random\n","import config\n","import os\n","import yaml\n","import logging\n","import difflib\n","import pandas as pd\n","\n","import transformers\n","import datasets\n","import torch\n","\n","from tqdm import tqdm\n","from utilities import *\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","logger = logging.getLogger(__name__)\n","global_config = None"],"metadata":{"id":"A5CuneHIhWEZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = datasets.load_dataset(\"lamini/lamini_docs\")\n","\n","test_dataset = dataset[\"test\"]"],"metadata":{"id":"qUcEx5e6hYts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(test_dataset[0][\"question\"])\n","print(test_dataset[0][\"answer\"])"],"metadata":{"id":"_5_EO76khZxK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"lamini/lamini_docs_finetuned\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)"],"metadata":{"id":"UHCxHitihard"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Setup a really basic evaluation function"],"metadata":{"id":"TBN4zQlDhb8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def is_exact_match(a, b):\n","    return a.strip() == b.strip()"],"metadata":{"id":"e337QPQ5hcwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()"],"metadata":{"id":"gmLsq5mrhdo3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n","  # Tokenize\n","  tokenizer.pad_token = tokenizer.eos_token\n","  input_ids = tokenizer.encode(\n","      text,\n","      return_tensors=\"pt\",\n","      truncation=True,\n","      max_length=max_input_tokens\n","  )\n","\n","  # Generate\n","  device = model.device\n","  generated_tokens_with_prompt = model.generate(\n","    input_ids=input_ids.to(device),\n","    max_length=max_output_tokens\n","  )\n","\n","  # Decode\n","  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n","\n","  # Strip the prompt\n","  generated_text_answer = generated_text_with_prompt[0][len(text):]\n","\n","  return generated_text_answer"],"metadata":{"id":"ocEp9KPOhetT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Run model and compare to expected answer"],"metadata":{"id":"Pe-_llPthgOh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_question = test_dataset[0][\"question\"]\n","generated_answer = inference(test_question, model, tokenizer)\n","print(test_question)\n","print(generated_answer)"],"metadata":{"id":"4C6EDfROhhwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["answer = test_dataset[0][\"answer\"]\n","print(answer)"],"metadata":{"id":"09T-A51thjo_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["exact_match = is_exact_match(generated_answer, answer)\n","print(exact_match)"],"metadata":{"id":"nE_imBDchlL8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Run over entire dataset"],"metadata":{"id":"RbSDNKbthnNf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = 10\n","metrics = {'exact_matches': []}\n","predictions = []\n","for i, item in tqdm(enumerate(test_dataset)):\n","    print(\"i Evaluating: \" + str(item))\n","    question = item['question']\n","    answer = item['answer']\n","\n","    try:\n","      predicted_answer = inference(question, model, tokenizer)\n","    except:\n","      continue\n","    predictions.append([predicted_answer, answer])\n","\n","    #fixed: exact_match = is_exact_match(generated_answer, answer)\n","    exact_match = is_exact_match(predicted_answer, answer)\n","    metrics['exact_matches'].append(exact_match)\n","\n","    if i > n and n != -1:\n","      break\n","print('Number of exact matches: ', sum(metrics['exact_matches']))"],"metadata":{"id":"2vIlTEJ9holO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame(predictions, columns=[\"predicted_answer\", \"target_answer\"])\n","print(df)"],"metadata":{"id":"sxc7_lgshqVd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Evaluate all the data"],"metadata":{"id":"Be5O0e6Threl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluation_dataset_path = \"lamini/lamini_docs_evaluation\"\n","evaluation_dataset = datasets.load_dataset(evaluation_dataset_path)"],"metadata":{"id":"h1To_Z_9htKg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(evaluation_dataset)"],"metadata":{"id":"f4cA_tNuhvjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Try the ARC benchmark\n","This can take several minutes"],"metadata":{"id":"0CegwQochxZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python lm-evaluation-harness/main.py --model hf-causal --model_args pretrained=lamini/lamini_docs_finetuned --tasks arc_easy --device cpu"],"metadata":{"id":"Qt7yLO0YhzBa"},"execution_count":null,"outputs":[]}]}