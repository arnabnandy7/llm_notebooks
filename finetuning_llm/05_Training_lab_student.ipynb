{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDpXBhxgxkZzcP3OVIeEcO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aIX7jqlwb1nA"},"outputs":[],"source":["# Training"]},{"cell_type":"code","source":["## Technically, it's only a few lines of code to run on GPUs (elsewhere, ie. on Lamini).\n","```\n","from llama import BasicModelRunner\n","\n","model = BasicModelRunner(\"EleutherAI/pythia-410m\")\n","model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n","model.train(is_public=True)\n","\n","\n","```\n","1. Choose base model.\n","2. Load data.\n","3. Train it. Returns a model ID, dashboard, and playground interface.\n","\n","### Let's look under the hood at the core code running this! This is the open core of Lamini's `llama` library :)"],"metadata":{"id":"d6GrbyACb7df"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datasets\n","import tempfile\n","import logging\n","import random\n","import config\n","import os\n","import yaml\n","import time\n","import torch\n","import transformers\n","import pandas as pd\n","import jsonlines\n","\n","from utilities import *\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForCausalLM\n","from transformers import TrainingArguments\n","from transformers import AutoModelForCausalLM\n","from llama import BasicModelRunner\n","\n","\n","logger = logging.getLogger(__name__)\n","global_config = None"],"metadata":{"id":"7dv-Xlf1b9Pm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Load the Lamini docs dataset"],"metadata":{"id":"hN1fQH7yb-73"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_name = \"lamini_docs.jsonl\"\n","dataset_path = f\"/content/{dataset_name}\"\n","use_hf = False"],"metadata":{"id":"fOE7PklGcAbM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_path = \"lamini/lamini_docs\"\n","use_hf = True"],"metadata":{"id":"OGK50SjScCA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Set up the model, training config, and tokenizer"],"metadata":{"id":"uiRzqZWrcDVl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"EleutherAI/pythia-70m\""],"metadata":{"id":"4xgXG04zcEfX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_config = {\n","    \"model\": {\n","        \"pretrained_name\": model_name,\n","        \"max_length\" : 2048\n","    },\n","    \"datasets\": {\n","        \"use_hf\": use_hf,\n","        \"path\": dataset_path\n","    },\n","    \"verbose\": True\n","}"],"metadata":{"id":"RaaBf4kVcF5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n","\n","print(train_dataset)\n","print(test_dataset)"],"metadata":{"id":"5lzF5BGkcHpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Load the base model"],"metadata":{"id":"GK0z6PnhcI0G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model = AutoModelForCausalLM.from_pretrained(model_name)"],"metadata":{"id":"NYHib6vFcJyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device_count = torch.cuda.device_count()\n","if device_count > 0:\n","    logger.debug(\"Select GPU device\")\n","    device = torch.device(\"cuda\")\n","else:\n","    logger.debug(\"Select CPU device\")\n","    device = torch.device(\"cpu\")"],"metadata":{"id":"RXB8R7tvcLNl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model.to(device)"],"metadata":{"id":"rXedIyhPcNXt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Define function to carry out inference"],"metadata":{"id":"v15vnfOocOxY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n","  # Tokenize\n","  input_ids = tokenizer.encode(\n","          text,\n","          return_tensors=\"pt\",\n","          truncation=True,\n","          max_length=max_input_tokens\n","  )\n","\n","  # Generate\n","  device = model.device\n","  generated_tokens_with_prompt = model.generate(\n","    input_ids=input_ids.to(device),\n","    max_length=max_output_tokens\n","  )\n","\n","  # Decode\n","  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n","\n","  # Strip the prompt\n","  generated_text_answer = generated_text_with_prompt[0][len(text):]\n","\n","  return generated_text_answer"],"metadata":{"id":"T2fQ2eGDcP7V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Try the base model"],"metadata":{"id":"SO7slOofcSdv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_text = test_dataset[0]['question']\n","print(\"Question input (test):\", test_text)\n","print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n","print(\"Model's answer: \")\n","print(inference(test_text, base_model, tokenizer))"],"metadata":{"id":"ZLlwwWzMcTVZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Setup training"],"metadata":{"id":"0UuKID5acUhA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_steps = 3"],"metadata":{"id":"uq96JKZ5cVl1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n","output_dir = trained_model_name"],"metadata":{"id":"cQ30Z1UpcWe-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","\n","  # Learning rate\n","  learning_rate=1.0e-5,\n","\n","  # Number of training epochs\n","  num_train_epochs=1,\n","\n","  # Max steps to train for (each step is a batch of data)\n","  # Overrides num_train_epochs, if not -1\n","  max_steps=max_steps,\n","\n","  # Batch size for training\n","  per_device_train_batch_size=1,\n","\n","  # Directory to save model checkpoints\n","  output_dir=output_dir,\n","\n","  # Other arguments\n","  overwrite_output_dir=False, # Overwrite the content of the output directory\n","  disable_tqdm=False, # Disable progress bars\n","  eval_steps=120, # Number of update steps between two evaluations\n","  save_steps=120, # After # steps model is saved\n","  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n","  per_device_eval_batch_size=1, # Batch size for evaluation\n","  evaluation_strategy=\"steps\",\n","  logging_strategy=\"steps\",\n","  logging_steps=1,\n","  optim=\"adafactor\",\n","  gradient_accumulation_steps = 4,\n","  gradient_checkpointing=False,\n","\n","  # Parameters for early stopping\n","  load_best_model_at_end=True,\n","  save_total_limit=1,\n","  metric_for_best_model=\"eval_loss\",\n","  greater_is_better=False\n",")"],"metadata":{"id":"GsA-8jcpcYEX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_flops = (\n","  base_model.floating_point_ops(\n","    {\n","       \"input_ids\": torch.zeros(\n","           (1, training_config[\"model\"][\"max_length\"])\n","      )\n","    }\n","  )\n","  * training_args.gradient_accumulation_steps\n",")\n","\n","print(base_model)\n","print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n","print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"],"metadata":{"id":"BWG0ibANcZLG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=base_model,\n","    model_flops=model_flops,\n","    total_steps=max_steps,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")"],"metadata":{"id":"tm87E0lhca5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Train a few steps"],"metadata":{"id":"HeZQOzqbccUa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_output = trainer.train()"],"metadata":{"id":"sRXjf5ZFcdWW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Save model locally"],"metadata":{"id":"0lrNtdkocehr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_dir = f'{output_dir}/final'\n","\n","trainer.save_model(save_dir)\n","print(\"Saved model to:\", save_dir)"],"metadata":{"id":"_R3Fd7qecf9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n"],"metadata":{"id":"vIrRsJ9lchMC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["finetuned_slightly_model.to(device)\n"],"metadata":{"id":"hL_bprFQcjPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Run slightly trained model"],"metadata":{"id":"MDVcdNbmckbW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_question = test_dataset[0]['question']\n","print(\"Question input (test):\", test_question)\n","\n","print(\"Finetuned slightly model's answer: \")\n","print(inference(test_question, finetuned_slightly_model, tokenizer))"],"metadata":{"id":"cj9xwSpfclgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_answer = test_dataset[0]['answer']\n","print(\"Target answer output (test):\", test_answer)"],"metadata":{"id":"Xyq18l6dcnBd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Run same model trained for two epochs"],"metadata":{"id":"ZbsYMiGQcoML"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n","tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n","\n","finetuned_longer_model.to(device)\n","print(\"Finetuned longer model's answer: \")\n","print(inference(test_question, finetuned_longer_model, tokenizer))"],"metadata":{"id":"3-49tG2JcpYW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Run much larger trained model and explore moderation"],"metadata":{"id":"ZGIi0YtNcran"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bigger_finetuned_model = BasicModelRunner(model_name_to_id[\"bigger_model_name\"])\n","bigger_finetuned_output = bigger_finetuned_model(test_question)\n","print(\"Bigger (2.8B) finetuned model (test): \", bigger_finetuned_output)"],"metadata":{"id":"IpRbpdl-csuv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","for i in range(len(train_dataset)):\n"," if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n","  print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n","  count += 1\n","print(count)"],"metadata":{"id":"Y2G_5fbwct88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Explore moderation using small model\n","First, try the non-finetuned base model:"],"metadata":{"id":"l2ab8S1hcwR6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n","base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n","print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"],"metadata":{"id":"x1IhEBWBcxQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Now try moderation with finetuned small model"],"metadata":{"id":"rdU4m4Q-cyYA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(inference(\"What do you think of Mars?\", finetuned_longer_model, tokenizer))"],"metadata":{"id":"C2MQmE_Icz6L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Finetune a model in 3 lines of code using Lamini"],"metadata":{"id":"r5LWH9djc0xT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BasicModelRunner(\"EleutherAI/pythia-410m\")\n","model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n","model.train(is_public=True)"],"metadata":{"id":"0lAF8rl7c2gz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out = model.evaluate()"],"metadata":{"id":"43J_xq61c3gw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lofd = []\n","for e in out['eval_results']:\n","    q  = f\"{e['input']}\"\n","    at = f\"{e['outputs'][0]['output']}\"\n","    ab = f\"{e['outputs'][1]['output']}\"\n","    di = {'question': q, 'trained model': at, 'Base Model' : ab}\n","    lofd.append(di)\n","df = pd.DataFrame.from_dict(lofd)\n","style_df = df.style.set_properties(**{'text-align': 'left'})\n","style_df = style_df.set_properties(**{\"vertical-align\": \"text-top\"})\n","style_df"],"metadata":{"id":"jrInbf_oc4gx"},"execution_count":null,"outputs":[]}]}