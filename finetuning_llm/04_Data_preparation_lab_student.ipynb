{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMwQQxLnaaFsqa3YkMJrJbQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pFoiWWP0Svtm"},"outputs":[],"source":["# Data preparation"]},{"cell_type":"code","source":["import pandas as pd\n","import datasets\n","\n","from pprint import pprint\n","from transformers import AutoTokenizer"],"metadata":{"id":"vlpuWI6wVAne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Tokenizing text"],"metadata":{"id":"pET5QL5BVCJA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"],"metadata":{"id":"HwdkYa7DVDri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"Hi, how are you?\""],"metadata":{"id":"fQDx5TlgVFeo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_text = tokenizer(text)[\"input_ids\"]"],"metadata":{"id":"7Ar9OuwJVHf0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_text"],"metadata":{"id":"hNGqr_ZTVJWe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decoded_text = tokenizer.decode(encoded_text)\n","print(\"Decoded tokens back into text: \", decoded_text)"],"metadata":{"id":"xuJrb5aeVLfi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Tokenize multiple texts at once"],"metadata":{"id":"cVefaWXJVNAQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n","encoded_texts = tokenizer(list_texts)\n","print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"],"metadata":{"id":"7368QOWOVOFk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Padding and truncation"],"metadata":{"id":"X038JyshVPFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.pad_token = tokenizer.eos_token\n","encoded_texts_longest = tokenizer(list_texts, padding=True)\n","print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"],"metadata":{"id":"fLa-VsE5VQSC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n","print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"],"metadata":{"id":"_krgzs8yVRXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.truncation_side = \"left\"\n","encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n","print(\"Using left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])"],"metadata":{"id":"HDgYlvolVUHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n","print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"],"metadata":{"id":"-FaFYEKbVVdI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Prepare instruction dataset"],"metadata":{"id":"IAkCltiAVWxJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","filename = \"lamini_docs.jsonl\"\n","instruction_dataset_df = pd.read_json(filename, lines=True)\n","examples = instruction_dataset_df.to_dict()\n","\n","if \"question\" in examples and \"answer\" in examples:\n","  text = examples[\"question\"][0] + examples[\"answer\"][0]\n","elif \"instruction\" in examples and \"response\" in examples:\n","  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n","elif \"input\" in examples and \"output\" in examples:\n","  text = examples[\"input\"][0] + examples[\"output\"][0]\n","else:\n","  text = examples[\"text\"][0]\n","\n","prompt_template = \"\"\"### Question:\n","{question}\n","\n","### Answer:\"\"\"\n","\n","num_examples = len(examples[\"question\"])\n","finetuning_dataset = []\n","for i in range(num_examples):\n","  question = examples[\"question\"][i]\n","  answer = examples[\"answer\"][i]\n","  text_with_prompt_template = prompt_template.format(question=question)\n","  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n","\n","from pprint import pprint\n","print(\"One datapoint in the finetuning dataset:\")\n","pprint(finetuning_dataset[0])"],"metadata":{"id":"KVxsU7ksVYZ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Tokenize a single example"],"metadata":{"id":"TY-ZxMZfVbIl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n","tokenized_inputs = tokenizer(\n","    text,\n","    return_tensors=\"np\",\n","    padding=True\n",")\n","print(tokenized_inputs[\"input_ids\"])"],"metadata":{"id":"IvQDJxwHVc1Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_length = 2048\n","max_length = min(\n","    tokenized_inputs[\"input_ids\"].shape[1],\n","    max_length,\n",")"],"metadata":{"id":"yjSsP4D2VgG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_inputs = tokenizer(\n","    text,\n","    return_tensors=\"np\",\n","    truncation=True,\n","    max_length=max_length\n",")"],"metadata":{"id":"wk8Ty1wAVhtZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_inputs[\"input_ids\"]"],"metadata":{"id":"5_gOI01DVitf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Tokenize the instruction dataset"],"metadata":{"id":"4m-78_zLVjq0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_function(examples):\n","    if \"question\" in examples and \"answer\" in examples:\n","      text = examples[\"question\"][0] + examples[\"answer\"][0]\n","    elif \"input\" in examples and \"output\" in examples:\n","      text = examples[\"input\"][0] + examples[\"output\"][0]\n","    else:\n","      text = examples[\"text\"][0]\n","\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenized_inputs = tokenizer(\n","        text,\n","        return_tensors=\"np\",\n","        padding=True,\n","    )\n","\n","    max_length = min(\n","        tokenized_inputs[\"input_ids\"].shape[1],\n","        2048\n","    )\n","    tokenizer.truncation_side = \"left\"\n","    tokenized_inputs = tokenizer(\n","        text,\n","        return_tensors=\"np\",\n","        truncation=True,\n","        max_length=max_length\n","    )\n","\n","    return tokenized_inputs"],"metadata":{"id":"BsuN8S91VkwM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=filename, split=\"train\")\n","\n","tokenized_dataset = finetuning_dataset_loaded.map(\n","    tokenize_function,\n","    batched=True,\n","    batch_size=1,\n","    drop_last_batch=True\n",")\n","\n","print(tokenized_dataset)"],"metadata":{"id":"HeVRoY3aVmF5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"],"metadata":{"id":"S9uxuZ2oVn8_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Prepare test/train splits"],"metadata":{"id":"6rjlcG1DVpUP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n","print(split_dataset)"],"metadata":{"id":"tRoaaSl2VqnZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Some datasets for you to try"],"metadata":{"id":"uFgAX70zVr21"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["finetuning_dataset_path = \"lamini/lamini_docs\"\n","finetuning_dataset = datasets.load_dataset(finetuning_dataset_path)\n","print(finetuning_dataset)"],"metadata":{"id":"0Lm4vXTAVtDP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["taylor_swift_dataset = \"lamini/taylor_swift\"\n","bts_dataset = \"lamini/bts\"\n","open_llms = \"lamini/open_llms\""],"metadata":{"id":"6942wB77VuvK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_swiftie = datasets.load_dataset(taylor_swift_dataset)\n","print(dataset_swiftie[\"train\"][1])"],"metadata":{"id":"TtRvSx8QVvyh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This is how to push your own dataset to your Huggingface hub\n","# !pip install huggingface_hub\n","# !huggingface-cli login\n","# split_dataset.push_to_hub(dataset_path_hf)"],"metadata":{"id":"_wRoYAKgVwx0"},"execution_count":null,"outputs":[]}]}