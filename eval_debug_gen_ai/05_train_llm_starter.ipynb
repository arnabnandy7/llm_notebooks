{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDYxT1NtQ12bI2H9rS/2ym"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Nd7bJYDW_2Bl"},"outputs":[],"source":["# Finetuning a language model\n","\n","<!--- @wandbcode{dlai_05} -->\n","\n","Let's see how to finetune a language model to generate character backstories using HuggingFace Trainer with wandb integration. We'll use a tiny language model (`TinyStories-33M`) due to resource constraints, but the lessons you learn here should be applicable to large models too!"]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","from datasets import load_dataset\n","from transformers import AutoModelForCausalLM\n","from transformers import Trainer, TrainingArguments\n","import transformers\n","transformers.set_seed(42)\n","\n","import wandb"],"metadata":{"id":"gurGgZ8OEjRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.login(anonymous=\"allow\")"],"metadata":{"id":"Umz7oV7tEmNO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_checkpoint = \"roneneldan/TinyStories-33M\""],"metadata":{"id":"lsi-Rd33EnUA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Preparing data\n","\n","We'll start by loading a dataset containing Dungeons and Dragons character biographies from Huggingface."],"metadata":{"id":"SXxqeAFcEosj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["> You can expect to get some warning here, this is ok"],"metadata":{"id":"c3UuXqnvEqE1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds = load_dataset('MohamedRashad/characters_backstories')"],"metadata":{"id":"G6MnKCaHErut"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's take a look at one example\n","ds[\"train\"][400]"],"metadata":{"id":"2RSUHy_pEs7_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# As this dataset has no validation split, we will create one\n","ds = ds[\"train\"].train_test_split(test_size=0.2, seed=42)"],"metadata":{"id":"hz_kcyuMEt4o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We'll create a tokenizer from model checkpoint\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n","\n","# We'll need padding to have same length sequences in a batch\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Define a tokenization function that first concatenates text and target\n","def tokenize_function(example):\n","    merged = example[\"text\"] + \" \" + example[\"target\"]\n","    batch = tokenizer(merged, padding='max_length', truncation=True, max_length=128)\n","    batch[\"labels\"] = batch[\"input_ids\"].copy()\n","    return batch\n","\n","# Apply it on our dataset, and remove the text columns\n","tokenized_datasets = ds.map(tokenize_function, remove_columns=[\"text\", \"target\"])"],"metadata":{"id":"ZHmn_tmTEvAm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's check out one prepared example\n","print(tokenizer.decode(tokenized_datasets[\"train\"][900]['input_ids']))"],"metadata":{"id":"RjqCLHQDEwuu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Training\n","Let's finetune a pretrained language model on our dataset using HF Transformers and their wandb integration."],"metadata":{"id":"jVR363I2Ex0x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We will train a causal (autoregressive) language model from a pretrained checkpoint\n","model = AutoModelForCausalLM.from_pretrained(model_checkpoint);"],"metadata":{"id":"yrgqz2XZEzFf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Start a new wandb run\n","run = wandb.init(project='dlai_lm_tuning', job_type=\"training\", anonymous=\"allow\")"],"metadata":{"id":"TfVg5USJE0XX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define training arguments\n","model_name = model_checkpoint.split(\"/\")[-1]\n","training_args = TrainingArguments(\n","    f\"{model_name}-finetuned-characters-backstories\",\n","    report_to=\"wandb\", # we need one line to track experiments in wandb\n","    num_train_epochs=1,\n","    logging_steps=1,\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=1e-4,\n","    weight_decay=0.01,\n","    no_cuda=True, # force cpu use, will be renamed `use_cpu`\n",")"],"metadata":{"id":"puLcx3CwE1Un"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We'll use HF Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n",")"],"metadata":{"id":"_1JqTllDE26v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's train!\n","trainer.train()"],"metadata":{"id":"iTaWAGJOE35E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformers.logging.set_verbosity_error() # suppress tokenizer warnings\n","\n","prefix = \"Generate Backstory based on following information Character Name: \"\n","\n","prompts = [\n","    \"Frogger Character Race: Aarakocra Character Class: Ranger Output: \",\n","    \"Smarty Character Race: Aasimar Character Class: Cleric Output: \",\n","    \"Volcano Character Race: Android Character Class: Paladin Output: \",\n","]\n","\n","table = wandb.Table(columns=[\"prompt\", \"generation\"])\n","\n","for prompt in prompts:\n","    input_ids = tokenizer.encode(prefix + prompt, return_tensors=\"pt\")\n","    output = model.generate(input_ids, do_sample=True, max_new_tokens=50, top_p=0.3)\n","    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","    table.add_data(prefix + prompt, output_text)\n","\n","wandb.log({'tiny_generations': table})"],"metadata":{"id":"Bb8YVHaFE5lR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["**Note**: LLM's don't always generate the same results. Your generated characters and backstories may differ from the video."],"metadata":{"id":"ZvJ7bV3QE7JV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.finish()"],"metadata":{"id":"8hhYRJ4qE8jY"},"execution_count":null,"outputs":[]}]}