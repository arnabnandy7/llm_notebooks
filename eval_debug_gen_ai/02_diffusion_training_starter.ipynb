{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYLPHwKo5vwB3s+pYlPu4R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-UIj-gCt5vKB"},"outputs":[],"source":["# Training a Diffusion Model with Weights and Biases (W&B)\n","\n","<!--- @wandbcode{dlai_02} -->\n","\n","In this notebooks we will instrument the training of a diffusion model with W&B. We will use the Lab3 notebook from the [\"How diffusion models work\"](https://www.deeplearning.ai/short-courses/how-diffusion-models-work/) course.\n","We will add:\n","- Logging of the training loss and metrics\n","- Sampling from the model during training and uploading the samples to W&B\n","- Saving the model checkpoints to W&B"]},{"cell_type":"code","source":["from types import SimpleNamespace\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from utilities import *\n","\n","import wandb"],"metadata":{"id":"Chiiq4S25zBl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.login(anonymous=\"allow\")"],"metadata":{"id":"9uIs9jLN50LR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Setting Things Up"],"metadata":{"id":"1oRY9-TM51wf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we are storing the parameters to be logged to wandb\n","DATA_DIR = Path('./data/')\n","SAVE_DIR = Path('./data/weights/')\n","SAVE_DIR.mkdir(exist_ok=True, parents=True)\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","config = SimpleNamespace(\n","    # hyperparameters\n","    num_samples = 30,\n","\n","    # diffusion hyperparameters\n","    timesteps = 500,\n","    beta1 = 1e-4,\n","    beta2 = 0.02,\n","\n","    # network hyperparameters\n","    n_feat = 64, # 64 hidden dimension feature\n","    n_cfeat = 5, # context vector is of size 5\n","    height = 16, # 16x16 image\n","\n","    # training hyperparameters\n","    batch_size = 100,\n","    n_epoch = 32,\n","    lrate = 1e-3,\n",")"],"metadata":{"id":"DZSlCppd522c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Setup DDPM noise scheduler and sampler (same as in the Diffusion course).\n","- perturb_input: Adds noise to the input image at the corresponding timestep on the schedule\n","- sample_ddpm_context: Generate images using the DDPM sampler, we will use this function during training to sample from the model regularly and see how our training is progressing"],"metadata":{"id":"j1YM8y6j549e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setup ddpm sampler functions\n","perturb_input, sample_ddpm_context = setup_ddpm(config.beta1,\n","                                                config.beta2,\n","                                                config.timesteps,\n","                                                DEVICE)"],"metadata":{"id":"XtISMtfU56c3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# construct model\n","nn_model = ContextUnet(in_channels=3,\n","                       n_feat=config.n_feat,\n","                       n_cfeat=config.n_cfeat,\n","                       height=config.height).to(DEVICE)"],"metadata":{"id":"WfzghdDL57vn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load dataset and construct optimizer\n","dataset = CustomDataset.from_np(path=DATA_DIR)\n","dataloader = DataLoader(dataset,\n","                        batch_size=config.batch_size,\n","                        shuffle=True)\n","optim = torch.optim.Adam(nn_model.parameters(), lr=config.lrate)"],"metadata":{"id":"bfqzIR9o59MB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Training"],"metadata":{"id":"g3NaHiHq5-0h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["We choose a fixed context vector with 6 samples of each class to guide our diffusion"],"metadata":{"id":"CSZq9wh65_0x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Noise vector\n","# x_T ~ N(0, 1), sample initial noise\n","noises = torch.randn(config.num_samples, 3,\n","                     config.height, config.height).to(DEVICE)\n","\n","# A fixed context vector to sample from\n","ctx_vector = F.one_hot(torch.tensor([0,0,0,0,0,0,   # hero\n","                                     1,1,1,1,1,1,   # non-hero\n","                                     2,2,2,2,2,2,   # food\n","                                     3,3,3,3,3,3,   # spell\n","                                     4,4,4,4,4,4]), # side-facing\n","                       5).to(DEVICE).float()"],"metadata":{"id":"8zjKB6DE6Ax3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["The following training cell takes very long to run on CPU, we have already trained the model for you on a GPU equipped machine.\n","\n","### You can visit the result of this >> [training here](https://wandb.ai/dlai-course/dlai_sprite_diffusion/runs/pzs3gsyo) <<"],"metadata":{"id":"IMkB2xv06CRz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a wandb run\n","run = wandb.init(project=\"dlai_sprite_diffusion\",\n","                 job_type=\"train\",\n","                 config=config)\n","\n","# we pass the config back from W&B\n","config = wandb.config\n","\n","for ep in tqdm(range(config.n_epoch), leave=True, total=config.n_epoch):\n","    # set into train mode\n","    nn_model.train()\n","    optim.param_groups[0]['lr'] = config.lrate*(1-ep/config.n_epoch)\n","\n","    pbar = tqdm(dataloader, leave=False)\n","    for x, c in pbar:   # x: images  c: context\n","        optim.zero_grad()\n","        x = x.to(DEVICE)\n","        c = c.to(DEVICE)\n","        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.8).to(DEVICE)\n","        c = c * context_mask.unsqueeze(-1)\n","        noise = torch.randn_like(x)\n","        t = torch.randint(1, config.timesteps + 1, (x.shape[0],)).to(DEVICE)\n","        x_pert = perturb_input(x, t, noise)\n","        pred_noise = nn_model(x_pert, t / config.timesteps, c=c)\n","        loss = F.mse_loss(pred_noise, noise)\n","        loss.backward()\n","        optim.step()\n","\n","        wandb.log({\"loss\": loss.item(),\n","                   \"lr\": optim.param_groups[0]['lr'],\n","                   \"epoch\": ep})\n","\n","\n","    # save model periodically\n","    if ep%4==0 or ep == int(config.n_epoch-1):\n","        nn_model.eval()\n","        ckpt_file = SAVE_DIR/f\"context_model.pth\"\n","        torch.save(nn_model.state_dict(), ckpt_file)\n","\n","        artifact_name = f\"{wandb.run.id}_context_model\"\n","        at = wandb.Artifact(artifact_name, type=\"model\")\n","        at.add_file(ckpt_file)\n","        wandb.log_artifact(at, aliases=[f\"epoch_{ep}\"])\n","\n","        samples, _ = sample_ddpm_context(nn_model,\n","                                         noises,\n","                                         ctx_vector[:config.num_samples])\n","        wandb.log({\n","            \"train_samples\": [\n","                wandb.Image(img) for img in samples.split(1)\n","            ]})\n","\n","# finish W&B run\n","wandb.finish()"],"metadata":{"id":"k1ZqMvcP6DXG"},"execution_count":null,"outputs":[]}]}