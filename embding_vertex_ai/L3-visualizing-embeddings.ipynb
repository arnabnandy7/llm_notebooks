{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNiz1TkFb59haRE7dDNap1h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_hyQe5-AR1Df"},"outputs":[],"source":["## Lesson 3: Visualizing Embeddings"]},{"cell_type":"code","source":["#### Project environment setup\n","\n","- Load credentials and relevant Python Libraries"],"metadata":{"id":"BPek0Pjjce_c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from utils import authenticate\n","credentials, PROJECT_ID = authenticate() #Get credentials and project ID"],"metadata":{"id":"14fXDwp9cfwy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["REGION = 'us-central1'"],"metadata":{"id":"hiQqK85dcgcX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### Enter project details"],"metadata":{"id":"mL6-ujrLchJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import and initialize the Vertex AI Python SDK\n","\n","import vertexai\n","vertexai.init(project=PROJECT_ID,\n","              location=REGION,\n","              credentials = credentials)"],"metadata":{"id":"ac3ufTXsch4t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Embeddings capture meaning"],"metadata":{"id":"oEwZkPLzcjgG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["in_1 = \"Missing flamingo discovered at swimming pool\"\n","\n","in_2 = \"Sea otter spotted on surfboard by beach\"\n","\n","in_3 = \"Baby panda enjoys boat ride\"\n","\n","\n","in_4 = \"Breakfast themed food truck beloved by all!\"\n","\n","in_5 = \"New curry restaurant aims to please!\"\n","\n","\n","in_6 = \"Python developers are wonderful people\"\n","\n","in_7 = \"TypeScript, C++ or Java? All are great!\"\n","\n","\n","input_text_lst_news = [in_1, in_2, in_3, in_4, in_5, in_6, in_7]"],"metadata":{"id":"_GKqTcpGck5W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from vertexai.language_models import TextEmbeddingModel\n","\n","embedding_model = TextEmbeddingModel.from_pretrained(\n","    \"textembedding-gecko@001\")"],"metadata":{"id":"02ScoaVrclzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["- Get embeddings for all pieces of text.\n","- Store them in a 2D NumPy array (one row for each embedding)."],"metadata":{"id":"2wObhCrncm8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings = []\n","for input_text in input_text_lst_news:\n","    emb = embedding_model.get_embeddings(\n","        [input_text])[0].values\n","    embeddings.append(emb)\n","\n","embeddings_array = np.array(embeddings)"],"metadata":{"id":"q8Ey4GTgcn9Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Shape: \" + str(embeddings_array.shape))\n","print(embeddings_array)"],"metadata":{"id":"XiALyvJjco6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### Reduce embeddings from 768 to 2 dimensions for visualization\n","- We'll use principal component analysis (PCA).\n","- You can learn more about PCA in [this video](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/73zWO/reducing-the-number-of-features-optional) from the Machine Learning Specialization."],"metadata":{"id":"nU8I63vUcp1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","\n","# Perform PCA for 2D visualization\n","PCA_model = PCA(n_components = 2)\n","PCA_model.fit(embeddings_array)\n","new_values = PCA_model.transform(embeddings_array)"],"metadata":{"id":"8ArVVmJ8cqxc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Shape: \" + str(new_values.shape))\n","print(new_values)"],"metadata":{"id":"mMcVhOkTcwtC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import mplcursors\n","%matplotlib ipympl\n","\n","from utils import plot_2D\n","plot_2D(new_values[:,0], new_values[:,1], input_text_lst_news)"],"metadata":{"id":"Uj6TEQ3acxp_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### Embeddings and Similarity\n","- Plot a heat map to compare the embeddings of sentences that are similar and sentences that are dissimilar."],"metadata":{"id":"zou8u2sscytC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["in_1 = \"\"\"He couldnâ€™t desert\n","          his post at the power plant.\"\"\"\n","\n","in_2 = \"\"\"The power plant needed\n","          him at the time.\"\"\"\n","\n","in_3 = \"\"\"Cacti are able to\n","          withstand dry environments.\"\"\"\n","\n","in_4 = \"\"\"Desert plants can\n","          survive droughts.\"\"\"\n","\n","input_text_lst_sim = [in_1, in_2, in_3, in_4]"],"metadata":{"id":"UdllbIVqczqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings = []\n","for input_text in input_text_lst_sim:\n","    emb = embedding_model.get_embeddings([input_text])[0].values\n","    embeddings.append(emb)\n","\n","embeddings_array = np.array(embeddings)"],"metadata":{"id":"UDzrgFSrc0kT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from utils import plot_heatmap\n","\n","y_labels = input_text_lst_sim\n","\n","# Plot the heatmap\n","plot_heatmap(embeddings_array, y_labels = y_labels, title = \"Embeddings Heatmap\")"],"metadata":{"id":"rLeVY7P3c1hu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Note: the heat map won't show everything because there are 768 columns to show.  To adjust the heat map with your mouse:\n","- Hover your mouse over the heat map.  Buttons will appear on the left of the heatmap.  Click on the button that has a vertical and horizontal double arrow (they look like axes).\n","- Left click and drag to move the heat map left and right.\n","- Right click and drag up to zoom in.\n","- Right click and drag down to zoom out."],"metadata":{"id":"P9NbFQCqc2dB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### Compute cosine similarity\n","- The `cosine_similarity` function expects a 2D array, which is why we'll wrap each embedding list inside another list.\n","- You can verify that sentence 1 and 2 have a higher similarity compared to sentence 1 and 4, even though sentence 1 and 4 both have the words \"desert\" and \"plant\"."],"metadata":{"id":"tiAors13c3XA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"hv23WNT5c4O5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compare(embeddings,idx1,idx2):\n","    return cosine_similarity([embeddings[idx1]],[embeddings[idx2]])"],"metadata":{"id":"nHA8vY-Bc5H7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(in_1)\n","print(in_2)\n","print(compare(embeddings,0,1))"],"metadata":{"id":"3wd4z07-c5_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(in_1)\n","print(in_4)\n","print(compare(embeddings,0,3))"],"metadata":{"id":"38LPcpvAc6rN"},"execution_count":null,"outputs":[]}]}